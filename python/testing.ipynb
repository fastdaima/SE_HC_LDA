{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirakr/miniconda3/envs/fastai_l/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens: 4183\n",
      "number of documents : 3227\n",
      "cluster no: 9\n",
      "0    1508\n",
      "1     327\n",
      "7     246\n",
      "4     222\n",
      "3     211\n",
      "2     194\n",
      "5     190\n",
      "6     187\n",
      "8     142\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from text_preprocessing import text_processing\n",
    "\n",
    "import string\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from nltk.corpus import *\n",
    "from nltk.tokenize import *\n",
    "import contractions\n",
    "from sklearn.feature_extraction import *\n",
    "from sklearn.metrics import *\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "def return_dict_corpus(train_df):\n",
    "    texts = list(train_df.lemmatize_title_w_pos.values)\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    print(f'number of unique tokens: {len(dictionary)}')\n",
    "    print(f'number of documents : {len(corpus)}')\n",
    "\n",
    "    return texts, dictionary, corpus\n",
    "\n",
    "\n",
    "path = Path(r'../Tawosi_Dataset')\n",
    "\n",
    "train, valid, test = pd.read_csv(path / 'DM-train.csv'), pd.read_csv(path / 'DM-valid.csv'), pd.read_csv(\n",
    "    path / 'DM-test.csv')\n",
    "\n",
    "data = pd.concat([train, valid])\n",
    "\n",
    "texts = []\n",
    "\n",
    "# print(train)\n",
    "\n",
    "train = text_processing(train)\n",
    "\n",
    "texts, dictionary, corpus = return_dict_corpus(train)\n",
    "\n",
    "num_topics = 20\n",
    "\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=100,\n",
    "    chunksize=2000,\n",
    "    iterations=400,\n",
    "    passes=20,\n",
    "    per_word_topics=True,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    eval_every=True\n",
    ")\n",
    "\n",
    "topics = lda_model[corpus]\n",
    "\n",
    "train_ik = train.issuekey.values\n",
    "\n",
    "\n",
    "test_prob = {}\n",
    "\n",
    "topics_number = set(f'topic_{i}' for i in range(num_topics))\n",
    "\n",
    "for key, prob in zip(train_ik, topics):\n",
    "    top_preds = {}\n",
    "\n",
    "    for (topic_no, value) in prob[0]:\n",
    "        top_preds[f'topic_{topic_no}'] = value\n",
    "\n",
    "\n",
    "    for tn in topics_number:\n",
    "        if not top_preds.get(tn, None):\n",
    "            top_preds[tn] = 0.0\n",
    "\n",
    "    test_prob[key] = top_preds\n",
    "\n",
    "\n",
    "prob_df_cols = sorted(list(topics_number), key=lambda x: int(x.split('_')[1]))\n",
    "\n",
    "prob_df = pd.DataFrame.from_dict(test_prob, orient='index')\n",
    "\n",
    "prob_df.index.name = 'issuekey'\n",
    "\n",
    "prob_df.to_csv('prob_df.csv')\n",
    "\n",
    "dendrogram = sch.dendrogram(sch.linkage(prob_df.values, method='ward'), no_plot=True)\n",
    "\n",
    "cn = len(set(dendrogram['color_list'])) - 1\n",
    "\n",
    "print(f\"cluster no: {cn}\")\n",
    "\n",
    "ac_m = AgglomerativeClustering(n_clusters=cn, affinity='euclidean', linkage='ward')\n",
    "\n",
    "preds = ac_m.fit_predict(prob_df.values)\n",
    "\n",
    "prob_df['labels'] = preds\n",
    "\n",
    "print(prob_df.labels.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens: 1936\n",
      "number of documents : 1078\n"
     ]
    }
   ],
   "source": [
    "test_df = text_processing(test)\n",
    "t_texts, t_dictionary, t_corpus = return_dict_corpus(test)\n",
    "t_topics = lda_model[t_corpus[0]]\n",
    "t_topics\n",
    "def return_prob_list(probs):\n",
    "    ind_prob = {}\n",
    "\n",
    "    for (topic_no, prob) in probs[0]:\n",
    "        ind_prob[f'topic_{topic_no}'] = prob\n",
    "\n",
    "    for tn in topics_number:\n",
    "        if not ind_prob.get(tn, None): ind_prob[tn] = 0.0\n",
    "\n",
    "    return ind_prob\n",
    "\n",
    "return_prob_list(t_topics)\n",
    "\n",
    "inp = list(dict(sorted(return_prob_list(t_topics).items(), key= lambda item: int(item[0].split('_')[1]))).values())\n",
    "inps = {\n",
    "    'inp-6767': return_prob_list(t_topics)\n",
    "}\n",
    "\n",
    "inp_df = pd.DataFrame.from_dict(inps, orient='index')\n",
    "inp_df.index.name = 'issue_key'\n",
    "# ac_m.fit_predict(inp_df.values)\n",
    "inp_df.values\n",
    "prob_df['index_number'] = [i for i in range(len(prob_df))]\n",
    "src_df = prob_df.drop(['labels', 'index_number'], axis=1)\n",
    "src_df.values\n",
    "arr2 = inp_df.values[0]\n",
    "euclidean_distance = {ind: np.linalg.norm(arr1 - arr2) for ind, arr1 in enumerate(src_df.values)}\n",
    "dict(sorted(euclidean_distance.items(), key=lambda x: float(x[1]), reverse=True))\n",
    "prob_df[prob_df.index_number==1429]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}